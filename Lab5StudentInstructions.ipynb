{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r38QRuzKqkQy"
   },
   "source": [
    "# Deep learning and AI methods\n",
    "## Session 5: Analyzing image data using convolution neural networks\n",
    "* Instructor: [Krzysztof Podgorski](https://krys.neocities.org),  [Statistics, Lund University, LUSEM](https://www.stat.lu.se/)\n",
    "* For more information visit the [CANVAS class website](https://canvas.education.lu.se/courses/1712).\n",
    "\n",
    "In this session you are ask to run the following Notebook in which convolution neural networks are used to analyse previously studied data set.\n",
    "\n",
    "### Main Task:\n",
    "\n",
    "Run the following code. Then explore on your own differented features of the presented tools. At the end of the notebook summarize what did you explore and draw some conclusions from your finding. This summary should not be longer than one page equivalent of a regular text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Image Classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts.\n",
    "\n",
    "It's okay if you don't understand everything. This is a fast-paced overview of a complete TensorFlow program, with explanations along the way. The goal is to get the general sense of a TensorFlow project, not to catch every detail.\n",
    "\n",
    "This guide uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0tMfX2vR0uD"
   },
   "source": [
    "## Install and import dependencies\n",
    "\n",
    "We'll need [TensorFlow Datasets](https://www.tensorflow.org/datasets/), an API that simplifies downloading and accessing datasets, and provides several sample datasets to work with. We're also using a few helper libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HDhfftMGc_i"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Use the %tensorflow_version magic if in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uusvhUp9Gg37"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bf08b18beffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import TensorFlow Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Helper libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Helper libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXZ44qIaG0Ru"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset fashion_mnist (29.45 MiB) to /Users/mats-ksp/tensorflow_datasets/fashion_mnist/1.0.0...\u001b[0m\n",
      "\u001b[1mDataset fashion_mnist downloaded and prepared to /Users/mats-ksp/tensorflow_datasets/fashion_mnist/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjnLH5S2CaWx"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES6uQoLKCaWr"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAsH3Zm-76pB"
   },
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "# The map function applies the normalize function to each element in the train\n",
    "# and test datasets\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "# The first time you use the dataset, the images will be loaded from disk\n",
    "# Caching will keep them in memory, making training faster\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIQbEiJGXM-q"
   },
   "source": [
    "### Check if the data are there\n",
    "\n",
    "Let's plot an image to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSzE9l7PjHx0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in test_dataset.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((28,28))\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples:     10000\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = metadata.splits['train'].num_examples\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples:     {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59veuiEZCaW4"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxg1XGm0eOBy"
   },
   "source": [
    "### Setup the layers\n",
    "\n",
    "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
    "\n",
    "Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gut8A_7rCaW6"
   },
   "source": [
    "This network layers are:\n",
    "\n",
    "* **\"convolutions\"** `tf.keras.layers.Conv2D and MaxPooling2D`— Network start with two pairs of Conv/MaxPool. The first layer is a Conv2D filters (3,3) being applied to the input image, retaining the original image size by using padding, and creating 32 output (convoluted) images (so this layer creates 32 convoluted images of the same size as input). After that, the 32 outputs are reduced in size using a MaxPooling2D (2,2) with a stride of 2. The next Conv2D also has a (3,3) kernel, takes the 32 images as input and creates 64 outputs which are again reduced in size by a MaxPooling2D layer. So far in the course, we have described what a Convolution does, but we haven't yet covered how you chain multiples of these together. We will get back to this in lesson 4 when we use color images. At this point, it's enough if you understand the kind of operation a convolutional filter performs\n",
    "\n",
    "* **output** `tf.keras.layers.Dense` — A 128-neuron, followed by 10-node *softmax* layer. Each node represents a class of clothing. As in the previous layer, the final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n",
    "\n",
    "\n",
    "### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
    "\n",
    "\n",
    "* *Loss function* — An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n",
    "* *Optimizer* —An algorithm for adjusting the inner parameters of the model in order to minimize loss.\n",
    "* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lhan11blCaW7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKF6uW-BCaW-"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, we define the iteration behavior for the train dataset:\n",
    "1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n",
    "2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n",
    "3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n",
    "\n",
    "Training is performed by calling the `model.fit` method:\n",
    "1. Feed the training data to the model using `train_dataset`.\n",
    "2. The model learns to associate images and labels.\n",
    "3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n",
    "\n",
    "(Don't worry about `steps_per_epoch`, the requirement to have this flag will soon be removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_Dp8971McQ1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvwvpA64CaW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.3899 - accuracy: 0.8587\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.2575 - accuracy: 0.9060\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2121 - accuracy: 0.9234\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.1824 - accuracy: 0.9323\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1516 - accuracy: 0.9435\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1302 - accuracy: 0.9527\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1116 - accuracy: 0.9591\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0926 - accuracy: 0.9661\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0783 - accuracy: 0.9710\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0635 - accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x62ab063c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3ZVOhugCaXA"
   },
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.97 (or 97%) on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEw4bZgGCaXB"
   },
   "source": [
    "## Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VflXLEeECaXC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3033 - accuracy: 0.9190\n",
      "Accuracy on test dataset: 0.919\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWfgsmVXCaXG"
   },
   "source": [
    "As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KtnHECKZni_"
   },
   "source": [
    "## Task 1:\n",
    "\n",
    "Experiment with different models and see how the accuracy results differ. In particular change the following parameters:\n",
    "*   Set training epochs set to 1\n",
    "*   Number of neurons in the Dense layer following the Flatten one. For example, go really low (e.g. 10) in ranges up to 512 and see how accuracy changes\n",
    "*   Add additional Dense layers between the Flatten and the final Dense(10, activation=tf.nn.softmax), experiment with different units in these layers\n",
    "*   Don't normalize the pixel values, and see the effect that has\n",
    "\n",
    "\n",
    "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
    "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
    "*   Edit -> Clear all outputs\n",
    "*   Runtime -> Reset all runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvogTtak_62F"
   },
   "source": [
    " Adjusting the epochs value for the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqVQGB1LiqRr"
   },
   "source": [
    "I create a new model with the same model architecture as above, as otherwise Python will remember the training. Thus, one could not get a proper insight on how adjusting the epochs value will affect the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5czE43FOjDIa"
   },
   "outputs": [],
   "source": [
    "model_alt = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "model_alt.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "Experiment with at least two more models that involve the convolution in one layer. Make the fit of your models and compare the accuracy betweem the model. Make predictions and explore the models. Illustrate your findings and comment. Comapare with the results to other methods explored in the previous labs on this data set. DO NOT experiment with cross-validation as it can be computationally too expensive for the convolution considered models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVVOzaoYAH8K"
   },
   "source": [
    "Adjusting the architecture of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vS59p1CdBul7"
   },
   "outputs": [],
   "source": [
    "#Set up different model architectures of the CNN\n",
    "#Model 2: 50 Units in the hidden layer\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fB1icMWRAwl"
   },
   "source": [
    "Adding additional dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlnRl0CARKYL"
   },
   "outputs": [],
   "source": [
    "#I play with different layers of a TOTAL of 128 hidden units\n",
    "#Model A is just the same as the first default model as specified already above, i.e. 1 hidden layer\n",
    "modelA = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "modelA.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTodgmkvU0o6"
   },
   "source": [
    "Unnormalized pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrfqXw4fVGSt"
   },
   "outputs": [],
   "source": [
    "#Download the data again, but this time storing it as dataset2\n",
    "dataset2, metadata2 = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset2, test_dataset2 = dataset2['train'], dataset2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xd435mgnVr4D"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPHGdD-NVtXL"
   },
   "outputs": [],
   "source": [
    "num_train_examples2 = metadata2.splits['train'].num_examples\n",
    "num_test_examples2 = metadata2.splits['test'].num_examples\n",
    "print(\"Number of training examples in dataset2: {}\".format(num_train_examples2))\n",
    "print(\"Number of test examples in dataset2:     {}\".format(num_test_examples2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCv_jD8DWT_s"
   },
   "source": [
    "For comparison reasons, I just train the unnormalized dataset with the very first model with 128 hidden units and one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIW8iYgdWJV-"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFc2HbEVCaXd"
   },
   "source": [
    "## Task 3: \n",
    "\n",
    "Use the reduced data set with 4 x 4 pixels and check the performance of your preferred convolution neural network on the reduced data set. Compare with all the previous results and comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader box: \n",
    "\n",
    "In what follows the grader will put the values according the following check list:\n",
    "\n",
    "* 1 Have all commands included in a raw notebook been evaluated? (0 or 0.5pt)\n",
    "* 2 Have all commands been experimented with? (0 or 0.5pt)\n",
    "* 3 Have all experiments been briefly commented? (0 or 0.5pt)\n",
    "* 4 Have all tasks been attempted? (0, 0.5, or 1pt)\n",
    "* 5 How many of the tasks have been completed? (0, 0.5, or 1pt)\n",
    "* 6 How many of the tasks (completed or not) have been commented? (0, 0.5, or 1pt)\n",
    "* 7 Have been the conclusions from performing the tasks clearly stated? (0, 0.5, or 1pt)\n",
    "* 8 Have been the overall organization of the submitted Lab notebook been neat and easy to follow by the grader? (0, or 0.5pt) \n",
    "\n",
    "\n",
    "#### 1 Have all commands included in a raw notebook been evaluated? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 2 Have all commands been experimented with? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 3 Have all experiments been briefly commented? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 4 Have all tasks been attempted? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 5 How many of the tasks have been completed? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 6 How many of the tasks (completed or not) have been commented? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 7 Have been the conclusions from performing the tasks clearly stated? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 8 Have been the overall organization of the submitted Lab notebook been neat and easy to follow by the grader? (0, or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "### Overall score\n",
    "\n",
    "### Score and grader's comment (if desired): \n",
    "N/A"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4Student_Buelent_v3.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
