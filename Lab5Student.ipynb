{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r38QRuzKqkQy"
   },
   "source": [
    "# Deep learning and AI methods\n",
    "## Session 5: Analyzing image data using convolution neural networks\n",
    "* Instructor: [Krzysztof Podgorski](https://krys.neocities.org),  [Statistics, Lund University, LUSEM](https://www.stat.lu.se/)\n",
    "* For more information visit the [CANVAS class website](https://canvas.education.lu.se/courses/1712).\n",
    "\n",
    "In this session you are ask to run the following Notebook in which convolution neural networks are used to analyse previously studied data set.\n",
    "\n",
    "### Main Task:\n",
    "\n",
    "Run the following code. Then explore on your own differented features of the presented tools. At the end of the notebook summarize what did you explore and draw some conclusions from your finding. This summary should not be longer than one page equivalent of a regular text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Image Classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts.\n",
    "\n",
    "It's okay if you don't understand everything. This is a fast-paced overview of a complete TensorFlow program, with explanations along the way. The goal is to get the general sense of a TensorFlow project, not to catch every detail.\n",
    "\n",
    "This guide uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0tMfX2vR0uD"
   },
   "source": [
    "## Install and import dependencies\n",
    "\n",
    "We'll need [TensorFlow Datasets](https://www.tensorflow.org/datasets/), an API that simplifies downloading and accessing datasets, and provides several sample datasets to work with. We're also using a few helper libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HDhfftMGc_i"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Use the %tensorflow_version magic if in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uusvhUp9Gg37"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Helper libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXZ44qIaG0Ru"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just something I need to set up to make fitting work without kernel dying\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjnLH5S2CaWx"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES6uQoLKCaWr"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAsH3Zm-76pB"
   },
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "# The map function applies the normalize function to each element in the train\n",
    "# and test datasets\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "# The first time you use the dataset, the images will be loaded from disk\n",
    "# Caching will keep them in memory, making training faster\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIQbEiJGXM-q"
   },
   "source": [
    "### Check if the data are there\n",
    "\n",
    "Let's plot an image to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSzE9l7PjHx0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in test_dataset.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((28,28))\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples:     10000\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = metadata.splits['train'].num_examples\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples:     {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59veuiEZCaW4"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxg1XGm0eOBy"
   },
   "source": [
    "### Setup the layers\n",
    "\n",
    "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
    "\n",
    "Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "# Setting up a model with 5 hidden layers of different types and nr of nodes\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gut8A_7rCaW6"
   },
   "source": [
    "This network layers are:\n",
    "\n",
    "* **\"convolutions\"** `tf.keras.layers.Conv2D and MaxPooling2D`— Network start with two pairs of Conv/MaxPool. The first layer is a Conv2D filters (3,3) being applied to the input image, retaining the original image size by using padding, and creating 32 output (convoluted) images (so this layer creates 32 convoluted images of the same size as input). After that, the 32 outputs are reduced in size using a MaxPooling2D (2,2) with a stride of 2. The next Conv2D also has a (3,3) kernel, takes the 32 images as input and creates 64 outputs which are again reduced in size by a MaxPooling2D layer. So far in the course, we have described what a Convolution does, but we haven't yet covered how you chain multiples of these together. We will get back to this in lesson 4 when we use color images. At this point, it's enough if you understand the kind of operation a convolutional filter performs\n",
    "\n",
    "* **output** `tf.keras.layers.Dense` — A 128-neuron, followed by 10-node *softmax* layer. Each node represents a class of clothing. As in the previous layer, the final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n",
    "\n",
    "\n",
    "### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
    "\n",
    "\n",
    "* *Loss function* — An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n",
    "* *Optimizer* —An algorithm for adjusting the inner parameters of the model in order to minimize loss.\n",
    "* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lhan11blCaW7"
   },
   "outputs": [],
   "source": [
    "# Setting up the same compiler as we've used before\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKF6uW-BCaW-"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, we define the iteration behavior for the train dataset:\n",
    "1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n",
    "2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n",
    "3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n",
    "\n",
    "Training is performed by calling the `model.fit` method:\n",
    "1. Feed the training data to the model using `train_dataset`.\n",
    "2. The model learns to associate images and labels.\n",
    "3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n",
    "\n",
    "(Don't worry about `steps_per_epoch`, the requirement to have this flag will soon be removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_Dp8971McQ1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Defining the PrintDot class in order to make the output more compact\n",
    "# while still being able to view the fitting process\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvwvpA64CaW_"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=10,\n",
    "          steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "          verbose=0,\n",
    "          callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3ZVOhugCaXA"
   },
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.97 (or 97%) on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Since I choose not to have the training process displayed, I can't actually see the train accuracy. Instead I should have saved the output from the fitting process into a history object, like we did in lab 4. I'll keep this in mind for the coming tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEw4bZgGCaXB"
   },
   "source": [
    "## Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VflXLEeECaXC"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWfgsmVXCaXG"
   },
   "source": [
    "As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KtnHECKZni_"
   },
   "source": [
    "## Task 1:\n",
    "\n",
    "Experiment with different models and see how the accuracy results differ. In particular change the following parameters:\n",
    "*   Set training epochs set to 1\n",
    "*   Number of neurons in the Dense layer following the Flatten one. For example, go really low (e.g. 10) in ranges up to 512 and see how accuracy changes\n",
    "*   Add additional Dense layers between the Flatten and the final Dense(10, activation=tf.nn.softmax), experiment with different units in these layers\n",
    "*   Don't normalize the pixel values, and see the effect that has\n",
    "\n",
    "\n",
    "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
    "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
    "*   Edit -> Clear all outputs\n",
    "*   Runtime -> Reset all runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvogTtak_62F"
   },
   "source": [
    " Adjusting the epochs value for the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqVQGB1LiqRr"
   },
   "source": [
    "I create a new model with the same model architecture as above, as otherwise Python will remember the training. Thus, one could not get a proper insight on how adjusting the epochs value will affect the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "So what I'm going to do now is experiment a little bit with some different parts of the neural networks. I'll mark the different experiments with new headlines so as to make it easier to follow what I'm doing. Some of it is code that was already included in this notebook, while some of it will be my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the number of epochs to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5czE43FOjDIa"
   },
   "outputs": [],
   "source": [
    "# Specifying a new model to see what happens when we train with 1 epoch\n",
    "model_new = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "model_new.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model with one epoch\n",
    "history1 = model_new.fit(train_dataset,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "print('\\nAccuracy:', history1.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss1, test_accuracy1 = model_new.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: The train accuracy with just one epoch is 85,78%, which is lower than before (97%). That makes sense, since the network basically wasn't given the chance to adjust it's predictions and get better at the task. It's still really high though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of neurons in the last hidden layer\n",
    "Here I'll try and see what happens when we adjust the number of nodes in the Dense layer that comes after the Faltten-layer in the original architecture. I'll use 5 epochs for training to save time since the models take quite long to run on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a model with only 10 nodes in the last hidden layer\n",
    "\n",
    "# Define model\n",
    "model10 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model10.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history10 = model10.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "# Test model\n",
    "test_loss10, test_accuracy10 = model10.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history10.history['acc'][np.max(history10.epoch)])\n",
    "print('Test accuracy:', test_accuracy10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a model with 220 nodes in the last hidden layer\n",
    "\n",
    "# Define model\n",
    "model220 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(220, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model220.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history220 = model220.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "# Test model\n",
    "test_loss220, test_accuracy220 = model220.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history220.history['acc'][np.max(history220.epoch)])\n",
    "print('Test accuracy:', test_accuracy220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a model with 512 nodes in the last hidden layer\n",
    "\n",
    "# Define model\n",
    "model512 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model512.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history512 = model512.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "# Test model\n",
    "test_loss512, test_accuracy512 = model512.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history512.history['acc'][np.max(history512.epoch)])\n",
    "print('Test accuracy:', test_accuracy512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy rates for comparison\n",
    "y = [test_accuracy10, test_accuracy220, test_accuracy512]\n",
    "x = ['10 nodes', '220 nodes', '512 nodes']\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: As we can see in the figure above, adjusting the node size doesn't really make much of a difference. More nodes does not mean higher accuracy for this task, which we can see if we look at the accuracy of the network with 512 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: Adding additional dense layers and trying on unnormalized data seems to be placed under the next task even though it was stated in task 1. So I will do them there, together with adjusting and experimenting with more models. In the end I will try to compare all of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "Experiment with at least two more models that involve the convolution in one layer. Make the fit of your models and compare the accuracy betweem the model. Make predictions and explore the models. Illustrate your findings and comment. Comapare with the results to other methods explored in the previous labs on this data set. DO NOT experiment with cross-validation as it can be computationally too expensive for the convolution considered models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVVOzaoYAH8K"
   },
   "source": [
    "#### Adjusting the architecture of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vS59p1CdBul7"
   },
   "outputs": [],
   "source": [
    "# Set up different model architectures of the CNN\n",
    "# Model 2: 50 Units in the hidden layer\n",
    "\n",
    "modelarch2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelarch2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyarch2 = modelarch2.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossarch2, test_accuracyarch2 = modelarch2.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyarch2.history['acc'][np.max(historyarch2.epoch)])\n",
    "print('Test accuracy:', test_accuracyarch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with one more pooling and Conv2D layer\n",
    "\n",
    "modelarch3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelarch3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyarch3 = modelarch3.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossarch3, test_accuracyarch3 = modelarch3.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyarch3.history['acc'][np.max(historyarch3.epoch)])\n",
    "print('Test accuracy:', test_accuracyarch3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with one less pooling and Conv2D layer\n",
    "\n",
    "modelarch4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelarch4.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyarch4 = modelarch4.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossarch4, test_accuracyarch4 = modelarch4.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyarch4.history['acc'][np.max(historyarch4.epoch)])\n",
    "print('Test accuracy:', test_accuracyarch4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy rates for comparison\n",
    "y = [test_accuracyarch4, test_accuracyarch2, test_accuracyarch3]\n",
    "x = ['1 Conv2D\\n1 Pooling', '2 Conv2D\\n2 Pooling', '3 Conv2D\\n3 Pooling']\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Again we can see that more complexity doeasn't really contribute to higher accuracy. Compared to the setup in the first model, we can even take away one set of Conv2D and Pooling layers and still achieve even slightly higher test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fB1icMWRAwl"
   },
   "source": [
    "#### Adding additional dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlnRl0CARKYL"
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers\n",
    "modelA = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelA.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyA = modelA.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossA, test_accuracyA = modelA.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyA.history['acc'][np.max(historyA.epoch)])\n",
    "print('Test accuracy:', test_accuracyA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 hidden layers\n",
    "modelB = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelB.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyB = modelB.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossB, test_accuracyB = modelB.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyB.history['acc'][np.max(historyB.epoch)])\n",
    "print('Test accuracy:', test_accuracyB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 hidden layers\n",
    "modelC = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelC.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyC = modelC.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossC, test_accuracyC = modelC.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyC.history['acc'][np.max(historyC.epoch)])\n",
    "print('Test accuracy:', test_accuracyC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy rates for comparison\n",
    "y = [test_accuracyA, test_accuracyB, test_accuracyC]\n",
    "x = ['2 hidden layers', '4 hidden layers', '8 hidden layers']\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Once again, adding complexity has a limit in contributing to accuracy. Accuracy does rise a bit when we use 4 layers though. At 8, it instead is lower than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTodgmkvU0o6"
   },
   "source": [
    "#### Unnormalized pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrfqXw4fVGSt"
   },
   "outputs": [],
   "source": [
    "#Download the data again, but this time storing it as dataset2\n",
    "dataset2, metadata2 = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset2, test_dataset2 = dataset2['train'], dataset2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xd435mgnVr4D"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPHGdD-NVtXL"
   },
   "outputs": [],
   "source": [
    "num_train_examples2 = metadata2.splits['train'].num_examples\n",
    "num_test_examples2 = metadata2.splits['test'].num_examples\n",
    "print(\"Number of training examples in dataset2: {}\".format(num_train_examples2))\n",
    "print(\"Number of test examples in dataset2:     {}\".format(num_test_examples2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in memory\n",
    "train_dataset2 =  train_dataset2.cache()\n",
    "test_dataset2  =  test_dataset2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split into batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataset2 = train_dataset2.repeat().shuffle(num_train_examples2).batch(BATCH_SIZE)\n",
    "test_dataset2 = test_dataset2.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCv_jD8DWT_s"
   },
   "source": [
    "For comparison reasons, I just train the unnormalized dataset with the very first model with 128 hidden units and one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIW8iYgdWJV-"
   },
   "outputs": [],
   "source": [
    "model_unscaled = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model_unscaled.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_unscaled = model_unscaled.fit(train_dataset2,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_loss_unscaled, test_accuracy_unscaled = model_unscaled.evaluate(test_dataset2, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history_unscaled.history['acc'][np.max(history_unscaled.epoch)])\n",
    "print('Test accuracy:', test_accuracy_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: I can't see much of a difference here, to be honest. But I know it's supposed to take more training for the network to find the right parameters. However, since I'm constantly working with 5 epochs to reduce the time it takes to train each network, that might be why I can't really see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFc2HbEVCaXd"
   },
   "source": [
    "## Task 3: \n",
    "\n",
    "Use the reduced data set with 4 x 4 pixels and check the performance of your preferred convolution neural network on the reduced data set. Compare with all the previous results and comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: In order to do this, I first need to load the data anew to get it into the dimensions that I've used on previous labs when reducing the number of dimensions. The I reduce the dimensions to 4, and then I run a model on that data.\n",
    "\n",
    "As you can see below, I've had troubles with the dimension reductions because of having an older version of tensorflow. I wasn't able to fix this before the deadline :( Hopefully you can still kind of see where I was aiming with my code :) Sorry for the mess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset3, metadata3 = tfds.load('fashion_mnist', as_supervised = True, with_info = True)\n",
    "train_dataset3, test_dataset3 = dataset3['train'], dataset3['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in memory\n",
    "train_dataset3 =  train_dataset3.cache()\n",
    "test_dataset3  =  test_dataset3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train_dataset3 =  train_dataset3.map(normalize)\n",
    "test_dataset3  =  test_dataset3.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating function for dimension reduction\n",
    "def reduce_size(images, labels):\n",
    "  images = tf.image.resize(images, [4,4])\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size to 4x4\n",
    "train_dataset_reduced = train_dataset3.map(reduce_size)\n",
    "test_dataset_reduced = test_dataset3.map(reduce_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in train_dataset_reduced.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((4,4))\n",
    "print(image)\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in test_dataset_reduced.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((4,4))\n",
    "print(image)\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: As we can see above, the size reduction isn't done in the same fashion as we've done previous labs. This shouldn't be a problem though, since it isn't stated in the task exactly how to reduce the dimensions. I'll stick with this option since it's still working code and that's what's most important here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and create batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataset_reduced = train_dataset_reduced.repeat().shuffle(metadata3.splits['train'].num_examples).batch(BATCH_SIZE)\n",
    "test_dataset_reduced = test_dataset_reduced.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Drawing on the experience from the previous experiments, I will build a model for the 4x4 data that is aimed to be as non-complex as possible. For example, I'll only use one Covd2D layer and one pooling layer. The last hidden layer will only have 20 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model4x4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(4, 4, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model4x4.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history4x4 = model4x4.fit(train_dataset_reduced,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(metadata3.splits['train'].num_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_loss4x4, test_accuracy4x4 = model4x4.evaluate(test_dataset_reduced, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history4x4.history['acc'][np.max(history4x4.epoch)])\n",
    "print('Test accuracy:', test_accuracy4x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the highest train accuracy amongst all models\n",
    "all_train_accuracy = [history.history['acc'],\n",
    "                      history1.history['acc'],\n",
    "                      history10.history['acc'][np.max(history10.epoch)],\n",
    "                      history220.history['acc'][np.max(history220.epoch)],\n",
    "                      history512.history['acc'][np.max(history512.epoch)],\n",
    "                      historyarch2.history['acc'][np.max(historyarch2.epoch)],\n",
    "                      historyarch3.history['acc'][np.max(historyarch3.epoch)],\n",
    "                      historyarch4.history['acc'][np.max(historyarch4.epoch)],\n",
    "                      historyA.history['acc'][np.max(historyA.epoch)],\n",
    "                      historyB.history['acc'][np.max(historyB.epoch)],\n",
    "                      historyC.history['acc'][np.max(historyC.epoch)],\n",
    "                      history_unscaled.history['acc'][np.max(history_unscaled.epoch)],\n",
    "                      history4x4.history['acc'][np.max(history4x4.epoch)]]\n",
    "\n",
    "print('Highest train accuracy was:', np.max(all_train_accuracy))\n",
    "print('It came from the model on list place:', np.argmax(all_train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a7e29728d926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Finding the highest test accuracy amongst all models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m all_test_accuracy = [test_accuracy,\n\u001b[0m\u001b[1;32m      3\u001b[0m                      \u001b[0mtest_accuracy1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0mtest_accuracy10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mtest_accuracy220\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Finding the highest test accuracy amongst all models\n",
    "all_test_accuracy = [test_accuracy,\n",
    "                     test_accuracy1,\n",
    "                     test_accuracy10,\n",
    "                     test_accuracy220,\n",
    "                     test_accuracy512,\n",
    "                     test_accuracyarch2,\n",
    "                     test_accuracyarch3,\n",
    "                     test_accuracyarch4,\n",
    "                     test_accuracyA,\n",
    "                     test_accuracyB,\n",
    "                     test_accuracyC,\n",
    "                     test_accuracy_unscaled,\n",
    "                     test_accuracy4x4]\n",
    "\n",
    "print('Highest test accuracy was:', np.max(all_test_accuracy))\n",
    "print('It came from the model on list place:', np.argmax(all_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary (as per requested as the main task)\n",
    "In this notebook I have built several deep learning models with the aim of classifying images of clothes. All models have performed with a very high accuracy. Due to an old version of tensorflow, I wasn't able to finish the last task but I have provided code that should work in a newer version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader box: \n",
    "\n",
    "In what follows the grader will put the values according the following check list:\n",
    "\n",
    "* 1 Have all commands included in a raw notebook been evaluated? (0 or 0.5pt)\n",
    "* 2 Have all commands been experimented with? (0 or 0.5pt)\n",
    "* 3 Have all experiments been briefly commented? (0 or 0.5pt)\n",
    "* 4 Have all tasks been attempted? (0, 0.5, or 1pt)\n",
    "* 5 How many of the tasks have been completed? (0, 0.5, or 1pt)\n",
    "* 6 How many of the tasks (completed or not) have been commented? (0, 0.5, or 1pt)\n",
    "* 7 Have been the conclusions from performing the tasks clearly stated? (0, 0.5, or 1pt)\n",
    "* 8 Have been the overall organization of the submitted Lab notebook been neat and easy to follow by the grader? (0, or 0.5pt) \n",
    "\n",
    "\n",
    "#### 1 Have all commands included in a raw notebook been evaluated? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 2 Have all commands been experimented with? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 3 Have all experiments been briefly commented? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 4 Have all tasks been attempted? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 5 How many of the tasks have been completed? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 6 How many of the tasks (completed or not) have been commented? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 7 Have been the conclusions from performing the tasks clearly stated? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 8 Have been the overall organization of the submitted Lab notebook been neat and easy to follow by the grader? (0, or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "### Overall score\n",
    "\n",
    "### Score and grader's comment (if desired): \n",
    "N/A"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4Student_Buelent_v3.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
