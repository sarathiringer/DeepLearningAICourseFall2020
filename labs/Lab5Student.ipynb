{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r38QRuzKqkQy"
   },
   "source": [
    "# Deep learning and AI methods\n",
    "## Session 5: Analyzing image data using convolution neural networks\n",
    "* Instructor: [Krzysztof Podgorski](https://krys.neocities.org),  [Statistics, Lund University, LUSEM](https://www.stat.lu.se/)\n",
    "* For more information visit the [CANVAS class website](https://canvas.education.lu.se/courses/1712).\n",
    "\n",
    "In this session you are ask to run the following Notebook in which convolution neural networks are used to analyse previously studied data set.\n",
    "\n",
    "### Main Task:\n",
    "\n",
    "Run the following code. Then explore on your own differented features of the presented tools. At the end of the notebook summarize what did you explore and draw some conclusions from your finding. This summary should not be longer than one page equivalent of a regular text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Image Classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts.\n",
    "\n",
    "It's okay if you don't understand everything. This is a fast-paced overview of a complete TensorFlow program, with explanations along the way. The goal is to get the general sense of a TensorFlow project, not to catch every detail.\n",
    "\n",
    "This guide uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0tMfX2vR0uD"
   },
   "source": [
    "## Install and import dependencies\n",
    "\n",
    "We'll need [TensorFlow Datasets](https://www.tensorflow.org/datasets/), an API that simplifies downloading and accessing datasets, and provides several sample datasets to work with. We're also using a few helper libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HDhfftMGc_i"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # Use the %tensorflow_version magic if in colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uusvhUp9Gg37"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Helper libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXZ44qIaG0Ru"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just something I need to set up to make fitting work without kernel dying\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjnLH5S2CaWx"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES6uQoLKCaWr"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAsH3Zm-76pB"
   },
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "# The map function applies the normalize function to each element in the train\n",
    "# and test datasets\n",
    "train_dataset =  train_dataset.map(normalize)\n",
    "test_dataset  =  test_dataset.map(normalize)\n",
    "\n",
    "# The first time you use the dataset, the images will be loaded from disk\n",
    "# Caching will keep them in memory, making training faster\n",
    "train_dataset =  train_dataset.cache()\n",
    "test_dataset  =  test_dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIQbEiJGXM-q"
   },
   "source": [
    "### Check if the data are there\n",
    "\n",
    "Let's plot an image to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSzE9l7PjHx0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in test_dataset.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((28,28))\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples:     10000\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = metadata.splits['train'].num_examples\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples:     {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59veuiEZCaW4"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxg1XGm0eOBy"
   },
   "source": [
    "### Setup the layers\n",
    "\n",
    "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
    "\n",
    "Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "# Setting up a model with 5 hidden layers of different types and nr of nodes\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gut8A_7rCaW6"
   },
   "source": [
    "This network layers are:\n",
    "\n",
    "* **\"convolutions\"** `tf.keras.layers.Conv2D and MaxPooling2D`— Network start with two pairs of Conv/MaxPool. The first layer is a Conv2D filters (3,3) being applied to the input image, retaining the original image size by using padding, and creating 32 output (convoluted) images (so this layer creates 32 convoluted images of the same size as input). After that, the 32 outputs are reduced in size using a MaxPooling2D (2,2) with a stride of 2. The next Conv2D also has a (3,3) kernel, takes the 32 images as input and creates 64 outputs which are again reduced in size by a MaxPooling2D layer. So far in the course, we have described what a Convolution does, but we haven't yet covered how you chain multiples of these together. We will get back to this in lesson 4 when we use color images. At this point, it's enough if you understand the kind of operation a convolutional filter performs\n",
    "\n",
    "* **output** `tf.keras.layers.Dense` — A 128-neuron, followed by 10-node *softmax* layer. Each node represents a class of clothing. As in the previous layer, the final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n",
    "\n",
    "\n",
    "### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
    "\n",
    "\n",
    "* *Loss function* — An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n",
    "* *Optimizer* —An algorithm for adjusting the inner parameters of the model in order to minimize loss.\n",
    "* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lhan11blCaW7"
   },
   "outputs": [],
   "source": [
    "# Setting up the same compiler as we've used before\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKF6uW-BCaW-"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "First, we define the iteration behavior for the train dataset:\n",
    "1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n",
    "2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n",
    "3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n",
    "\n",
    "Training is performed by calling the `model.fit` method:\n",
    "1. Feed the training data to the model using `train_dataset`.\n",
    "2. The model learns to associate images and labels.\n",
    "3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n",
    "\n",
    "(Don't worry about `steps_per_epoch`, the requirement to have this flag will soon be removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_Dp8971McQ1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Defining the PrintDot class in order to make the output more compact\n",
    "# while still being able to view the fitting process\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvwvpA64CaW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".........."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8db5bdd3d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=10,\n",
    "          steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "          verbose=0,\n",
    "          callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3ZVOhugCaXA"
   },
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.97 (or 97%) on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Since I choose not to have the training process displayed, I can't actually see the train accuracy. Instead I should have saved the output from the fitting process into a history object, like we did in lab 4. I'll keep this in mind for the coming tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEw4bZgGCaXB"
   },
   "source": [
    "## Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VflXLEeECaXC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 20s 64ms/step - loss: 0.2907 - accuracy: 0.9177\n",
      "Accuracy on test dataset: 0.9177\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWfgsmVXCaXG"
   },
   "source": [
    "As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KtnHECKZni_"
   },
   "source": [
    "## Task 1:\n",
    "\n",
    "Experiment with different models and see how the accuracy results differ. In particular change the following parameters:\n",
    "*   Set training epochs set to 1\n",
    "*   Number of neurons in the Dense layer following the Flatten one. For example, go really low (e.g. 10) in ranges up to 512 and see how accuracy changes\n",
    "*   Add additional Dense layers between the Flatten and the final Dense(10, activation=tf.nn.softmax), experiment with different units in these layers\n",
    "*   Don't normalize the pixel values, and see the effect that has\n",
    "\n",
    "\n",
    "Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n",
    "Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n",
    "*   Edit -> Clear all outputs\n",
    "*   Runtime -> Reset all runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvogTtak_62F"
   },
   "source": [
    " Adjusting the epochs value for the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqVQGB1LiqRr"
   },
   "source": [
    "I create a new model with the same model architecture as above, as otherwise Python will remember the training. Thus, one could not get a proper insight on how adjusting the epochs value will affect the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "So what I'm going to do now is experiment a little bit with some different parts of the neural networks. I'll mark the different experiments with new headlines so as to make it easier to follow what I'm doing. Some of it is code that was already included in this notebook, while some of it will be my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the number of epochs to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5czE43FOjDIa"
   },
   "outputs": [],
   "source": [
    "# Specifying a new model to see what happens when we train with 1 epoch\n",
    "model_new = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "model_new.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".\n",
      "Accuracy: [0.90595]\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model with one epoch\n",
    "history1 = model_new.fit(train_dataset,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "print('\\nAccuracy:', history1.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 23ms/step - loss: 0.2586 - accuracy: 0.9068\n",
      "Accuracy on test dataset: 0.9068\n"
     ]
    }
   ],
   "source": [
    "test_loss1, test_accuracy1 = model_new.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "print('Accuracy on test dataset:', test_accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: The train accuracy with just one epoch is 85,78%, which is lower than before (97%). That makes sense, since the network basically wasn't given the chance to adjust it's predictions and get better at the task. It's still really high though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of neurons in the last hidden layer\n",
    "Here I'll try and see what happens when we adjust the number of nodes in the Dense layer that comes after the Faltten-layer in the original architecture. I'll use 5 epochs for training to save time since the models take quite long to run on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 7s 24ms/step - loss: 0.2743 - accuracy: 0.9046\n",
      "\n",
      "Train accuracy: 0.91678333\n",
      "Test accuracy: 0.9046\n"
     ]
    }
   ],
   "source": [
    "# Training a model with only 10 nodes in the last hidden layer\n",
    "\n",
    "# Define model\n",
    "model10 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model10.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history10 = model10.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "# Test model\n",
    "test_loss10, test_accuracy10 = model10.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history10.history['accuracy'][np.max(history10.epoch)])\n",
    "print('Test accuracy:', test_accuracy10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 6s 21ms/step - loss: 0.2610 - accuracy: 0.9132\n",
      "\n",
      "Train accuracy: 0.9467667\n",
      "Test accuracy: 0.9132\n"
     ]
    }
   ],
   "source": [
    "# Training a model with 220 nodes in the last hidden layer\n",
    "\n",
    "# Define model\n",
    "model220 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(220, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model220.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history220 = model220.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "# Test model\n",
    "test_loss220, test_accuracy220 = model220.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history220.history['accuracy'][np.max(history220.epoch)])\n",
    "print('Test accuracy:', test_accuracy220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.2872 - accuracy: 0.8967\n",
      "\n",
      "Train accuracy: 0.91915\n",
      "Test accuracy: 0.8967\n"
     ]
    }
   ],
   "source": [
    "# Training a model with 512 nodes in the last hidden layer\n",
    "\n",
    "# Define model\n",
    "model512 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model512.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "history512 = model512.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "# Test model\n",
    "test_loss512, test_accuracy512 = model512.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history512.history['accuracy'][np.max(history512.epoch)])\n",
    "print('Test accuracy:', test_accuracy512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy rates for comparison\n",
    "y = [test_accuracy10, test_accuracy220, test_accuracy512]\n",
    "x = ['10 nodes', '220 nodes', '512 nodes']\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: As we can see in the figure above, adjusting the node size doesn't really make much of a difference. More nodes does not mean higher accuracy for this task, which we can see if we look at the accuracy of the network with 512 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: Adding additional dense layers and trying on unnormalized data seems to be placed under the next task even though it was stated in task 1. So I will do them there, together with adjusting and experimenting with more models. In the end I will try to compare all of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "Experiment with at least two more models that involve the convolution in one layer. Make the fit of your models and compare the accuracy betweem the model. Make predictions and explore the models. Illustrate your findings and comment. Comapare with the results to other methods explored in the previous labs on this data set. DO NOT experiment with cross-validation as it can be computationally too expensive for the convolution considered models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVVOzaoYAH8K"
   },
   "source": [
    "#### Adjusting the architecture of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vS59p1CdBul7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.2430 - accuracy: 0.9154\n",
      "\n",
      "Train accuracy: 0.93541664\n",
      "Test accuracy: 0.9154\n"
     ]
    }
   ],
   "source": [
    "# Set up different model architectures of the CNN\n",
    "# Model 2: 50 Units in the hidden layer\n",
    "\n",
    "modelarch2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelarch2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyarch2 = modelarch2.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossarch2, test_accuracyarch2 = modelarch2.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyarch2.history['accuracy'][np.max(historyarch2.epoch)])\n",
    "print('Test accuracy:', test_accuracyarch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2534 - accuracy: 0.9096\n",
      "\n",
      "Train accuracy: 0.92135\n",
      "Test accuracy: 0.9096\n"
     ]
    }
   ],
   "source": [
    "# Model with one more pooling and Conv2D layer\n",
    "\n",
    "modelarch3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelarch3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyarch3 = modelarch3.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossarch3, test_accuracyarch3 = modelarch3.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyarch3.history['accuracy'][np.max(historyarch3.epoch)])\n",
    "print('Test accuracy:', test_accuracyarch3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2702 - accuracy: 0.9072\n",
      "\n",
      "Train accuracy: 0.92838335\n",
      "Test accuracy: 0.9072\n"
     ]
    }
   ],
   "source": [
    "# Model with one less pooling and Conv2D layer\n",
    "\n",
    "modelarch4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelarch4.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyarch4 = modelarch4.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossarch4, test_accuracyarch4 = modelarch4.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyarch4.history['accuracy'][np.max(historyarch4.epoch)])\n",
    "print('Test accuracy:', test_accuracyarch4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEECAYAAAA4Qc+SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPIElEQVR4nO3df6zddX3H8efLFqYGB8RenBRYIcFpZwDlUojKRDdni8m6JdNRDW5Ebchk0/3IYEumM/yxMbexGcGuwUZdiI1TNpirkpkIbCFgi0Gg/FpTHBRIKOjccAuk8N4f56s7Hs6993tvT733fvp8JDc95/v9nHM+vZ/bZ7/93vs9TVUhSVr+XrTYE5AkTYZBl6RGGHRJaoRBl6RGGHRJaoRBl6RGrFysF161alWtWbNmsV5ekpalO+6448mqmhq3b9GCvmbNGnbt2rVYLy9Jy1KS/5hpn6dcJKkRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGrFoFxbp8LLmsn9e7Ck069t/9o7FnoKWCIMuaSz/Ej50DtVfwssy6H6hHToe7UnLl+fQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGtEr6EnWJ3kgyZ4kl43Zf3SSf0ryrSS7k1w0+alKkmYzZ9CTrACuAjYAa4FNSdaODPsgcG9VnQ6cB/xlkiMnPFdJ0iz6HKGvA/ZU1d6qehbYDmwcGVPAy5IEOAr4DnBgojOVJM2qT9BXA48M3d/XbRv2SeA1wGPA3cCHqur50SdKsjnJriS79u/fv8ApS5LG6RP0jNlWI/ffDtwJHA+cAXwyyU++4EFVW6tquqqmp6am5j1ZSdLM+gR9H3Di0P0TGByJD7sIuK4G9gAPAa+ezBQlSX30CfpO4NQkJ3ff6LwAuGFkzMPAzwMkeQXwM8DeSU5UkjS7lXMNqKoDSS4BbgRWANuqaneSi7v9W4DLgc8kuZvBKZpLq+rJQzhvSdKIOYMOUFU7gB0j27YM3X4M+MXJTk2SNB9eKSpJjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktSIXkFPsj7JA0n2JLlshjHnJbkzye4kN092mpKkuayca0CSFcBVwNuAfcDOJDdU1b1DY44BrgbWV9XDSY47VBOWJI3X5wh9HbCnqvZW1bPAdmDjyJh3A9dV1cMAVfXEZKcpSZpLn6CvBh4Zur+v2zbsVcCxSW5KckeS9457oiSbk+xKsmv//v0Lm7Ekaaw+Qc+YbTVyfyVwJvAO4O3AHyd51QseVLW1qqaranpqamrek5UkzWzOc+gMjshPHLp/AvDYmDFPVtX3ge8nuQU4HXhwIrOUJM2pzxH6TuDUJCcnORK4ALhhZMz1wLlJViZ5KXA2cN9kpypJms2cR+hVdSDJJcCNwApgW1XtTnJxt39LVd2X5KvAXcDzwDVVdc+hnLgk6Uf1OeVCVe0Adoxs2zJy/+PAxyc3NUnSfHilqCQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiN6BT3J+iQPJNmT5LJZxp2V5Lkkvzq5KUqS+pgz6ElWAFcBG4C1wKYka2cYdwVw46QnKUmaW58j9HXAnqraW1XPAtuBjWPG/RbwJeCJCc5PktRTn6CvBh4Zur+v2/ZDSVYDvwJsmdzUJEnz0SfoGbOtRu7/NXBpVT036xMlm5PsSrJr//79fecoSephZY8x+4ATh+6fADw2MmYa2J4EYBVwfpIDVfWPw4OqaiuwFWB6enr0LwVJ0kHoE/SdwKlJTgYeBS4A3j08oKpO/sHtJJ8Bvjwac0nSoTVn0KvqQJJLGPz0ygpgW1XtTnJxt9/z5pK0BPQ5QqeqdgA7RraNDXlV/cbBT0uSNF9eKSpJjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktSIXkFPsj7JA0n2JLlszP73JLmr+7g1yemTn6okaTZzBj3JCuAqYAOwFtiUZO3IsIeAN1fVacDlwNZJT1SSNLs+R+jrgD1VtbeqngW2AxuHB1TVrVX13e7ubcAJk52mJGkufYK+Gnhk6P6+bttM3gd85WAmJUmav5U9xmTMtho7MHkLg6C/aYb9m4HNACeddFLPKUqS+uhzhL4POHHo/gnAY6ODkpwGXANsrKqnxj1RVW2tqumqmp6amlrIfCVJM+gT9J3AqUlOTnIkcAFww/CAJCcB1wEXVtWDk5+mJGkuc55yqaoDSS4BbgRWANuqaneSi7v9W4CPAC8Hrk4CcKCqpg/dtCVJo/qcQ6eqdgA7RrZtGbr9fuD9k52aJGk+vFJUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRK+hJ1id5IMmeJJeN2Z8kn+j235Xk9ZOfqiRpNnMGPckK4CpgA7AW2JRk7ciwDcCp3cdm4FMTnqckaQ59jtDXAXuqam9VPQtsBzaOjNkIfK4GbgOOSfLKCc9VkjSLlT3GrAYeGbq/Dzi7x5jVwOPDg5JsZnAED/B0kgfmNdvlaxXw5GJPoo9csdgzWDJcs+Vl2awXHPSa/fRMO/oEPWO21QLGUFVbga09XrMpSXZV1fRiz0P9uWbLi+s10OeUyz7gxKH7JwCPLWCMJOkQ6hP0ncCpSU5OciRwAXDDyJgbgPd2P+1yDvC9qnp89IkkSYfOnKdcqupAkkuAG4EVwLaq2p3k4m7/FmAHcD6wB/gf4KJDN+Vl6bA7zdQA12x5cb2AVL3gVLckaRnySlFJaoRBl6RGGPROkm1Jnkhyzxzj3pvkniS7k9yb5PcnPI/3dG+fcFeSW5OcPrTvuSR3dq/9rSS/m+SwW8MkJyb5epL7us/Fh2YZ63otAUlenOQb3edhd5KPzTLWNVuoqvJj8H2EnwNeD9wzy5gNwDeB47v7LwY+MOF5vAE4duj1bh/a9/TQ7eOArwEfW+zP3SKs1SuB13e3XwY8CKx1vZbuB4NrVY7qbh8B3A6c45pN+PO82BNYSh/AmjmCfgvw1hn2nQHcBtwF/MPQF8xNwBXAN7rwnNttvx342aHH3wScOfKcxwKPDt1/emT/KcBTdN/cPlw/gOuBt7ley+MDeGkX7bNds8l+LJ9/SiwNrwXumGHf54BLq+o04G7go0P7VlbVOuDDQ9u3A+8C6N735viqGn3u9wFfmWkyVbWXwWmz4+b5+2hGkjXA6xj84R3lei0hSVYkuRN4AviXqnLNJsygT0CSo4FjqurmbtNnGZzC+YHrul/vYPCvAIAvAO/sbr8L+PuR53wLgy+2S+d6+YXNevlLchTwJeDDVfVf83ic67UIquq5qjqDwZXk65K8tu9jXbN+DPr87AbOXMDjnul+fY7uYq6qehR4KslpwK8xOJoAoNt2DbCxqp6a6UmTnNI95xMLmNOyluQIBjG/tqqum2GY67UEVdV/Mjj9sX7MbtfsIBj0+flT4M+T/BRAkp9I8ttV9T3gu0nO7cZdCNw805MM2Q78AXB0Vd3dPedJDI42LqyqB2d6YJIpYAvwyepO9h0ukgT4NHBfVf3VLENdryUiyVSSY7rbLwF+Abh/zFDX7CD0ebfFw0KSzwPnAauS7AM+WlWfHh5TVTuSvAL4WheVArZ1u38d2JLkpcBe+r39wReBvwEuH9r2EeDlwNWDl+BA/f+7yL2kOwd5BHAA+DtgtqC16o0M/kDf3X0+AP6oqnYMD3K9lpRXAp/N4D/MeRHwhar68ugg1+zgeOm/JDXCUy6S1AiDLkmNMOhD0uPy/yR/kuTR7vLge5L80gJf69tJVnW3b13onA9nfd8CwDVbGvpe/u96LZxB/1GfYfyPUo26svt52ncC2w72vR6q6g0H8/jD2AHg96rqNcA5wAeTrJ1hrGu2+J5hcBXo6Qyu+lyfwX+IM47rtQAGfUhV3QJ8Zx7j72MQlVVJNiW5uzui+OF/ATvT9mFJnu5+PS/JTUm+mOT+JNd23+knyfndtn9L8okkL/gJgcNNVT1eVd/sbv83cB+D/5x8tse4ZoukBp7u7h7Rfcz6Uxmu1/wY9IOQ5GzgeQZfmFcAb2Vw5HFWkl9Ocvy47XM87esYXL68lsH7SLwxyYuBvwU2VNWbgKlD8ftZzjL7WwAMj3PNFlH6Xf4/PN71mgeDvjC/031R/gWDK9CmgZuqan9VHQCuZXBZ8lkzbJ/NN6pqX1U9D9zJ4DLmVwN7q+qhbsznJ/47WsbS7y0AXLMlYB6X/7teC2DQF+bKqjqjqs6tqn9l5vd6WMh7QDwzdPsHlzEvm/eS+HFLv7cAANdsSZnj8n9wvRbEoE/G7cCbk6zK4Eq4TQwuS55p+3zdD5zSnVaAwRHLYa8799nnLQDGcc1+zNL/8v9xXK8evPR/SHpc/j9OVT2e5A+BrzP4m35HVV3fPefY7fNRVf+b5DeBryZ5ksH7PqvnWwCM45otil6X/4/jevXjpf/LRJKjqurp7qj0KuDfq+rKxZ6XZuaaLS8trJenXJaPD3RHobuBoxl8R15Lm2u2vCz79fIIXZIa4RG6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSI/4PQiW+l6ka810AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy rates for comparison\n",
    "y = [test_accuracyarch4, test_accuracyarch2, test_accuracyarch3]\n",
    "x = ['1 Conv2D\\n1 Pooling', '2 Conv2D\\n2 Pooling', '3 Conv2D\\n3 Pooling']\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Again we can see that more complexity doeasn't really contribute to higher accuracy. Compared to the setup in the first model, we can even take away one set of Conv2D and Pooling layers and still achieve the same test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fB1icMWRAwl"
   },
   "source": [
    "#### Adding additional dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlnRl0CARKYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.2382 - accuracy: 0.9132\n",
      "\n",
      "Train accuracy: 0.9332167\n",
      "Test accuracy: 0.9132\n"
     ]
    }
   ],
   "source": [
    "# 2 hidden layers\n",
    "modelA = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelA.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyA = modelA.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossA, test_accuracyA = modelA.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyA.history['accuracy'][np.max(historyA.epoch)])\n",
    "print('Test accuracy:', test_accuracyA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.2933 - accuracy: 0.8923\n",
      "\n",
      "Train accuracy: 0.91926664\n",
      "Test accuracy: 0.8923\n"
     ]
    }
   ],
   "source": [
    "# 4 hidden layers\n",
    "modelB = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelB.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyB = modelB.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossB, test_accuracyB = modelB.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyB.history['accuracy'][np.max(historyB.epoch)])\n",
    "print('Test accuracy:', test_accuracyB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.3225 - accuracy: 0.8866\n",
      "\n",
      "Train accuracy: 0.89073336\n",
      "Test accuracy: 0.8866\n"
     ]
    }
   ],
   "source": [
    "# 8 hidden layers\n",
    "modelC = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "modelC.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "historyC = modelC.fit(train_dataset,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_lossC, test_accuracyC = modelC.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', historyC.history['accuracy'][np.max(historyC.epoch)])\n",
    "print('Test accuracy:', test_accuracyC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the accuracy rates for comparison\n",
    "y = [test_accuracyA, test_accuracyB, test_accuracyC]\n",
    "x = ['2 hidden layers', '4 hidden layers', '8 hidden layers']\n",
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Once again, adding complexity has is not contributing to accuracy. Accuracy even decreases as we increade the number of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTodgmkvU0o6"
   },
   "source": [
    "#### Unnormalized pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrfqXw4fVGSt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "#Download the data again, but this time storing it as dataset2\n",
    "dataset2, metadata2 = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset2, test_dataset2 = dataset2['train'], dataset2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xd435mgnVr4D"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPHGdD-NVtXL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples in dataset2: 60000\n",
      "Number of test examples in dataset2:     10000\n"
     ]
    }
   ],
   "source": [
    "num_train_examples2 = metadata2.splits['train'].num_examples\n",
    "num_test_examples2 = metadata2.splits['test'].num_examples\n",
    "print(\"Number of training examples in dataset2: {}\".format(num_train_examples2))\n",
    "print(\"Number of test examples in dataset2:     {}\".format(num_test_examples2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in memory\n",
    "train_dataset2 =  train_dataset2.cache()\n",
    "test_dataset2  =  test_dataset2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split into batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataset2 = train_dataset2.repeat().shuffle(num_train_examples2).batch(BATCH_SIZE)\n",
    "test_dataset2 = test_dataset2.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCv_jD8DWT_s"
   },
   "source": [
    "For comparison reasons, I just train the unnormalized dataset with the very first model with 128 hidden units and one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIW8iYgdWJV-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 19s 60ms/step - loss: 0.3080 - accuracy: 0.9023\n",
      "\n",
      "Train accuracy: 0.92481667\n",
      "Test accuracy: 0.9023\n"
     ]
    }
   ],
   "source": [
    "model_unscaled = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model_unscaled.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_unscaled = model_unscaled.fit(train_dataset2,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_loss_unscaled, test_accuracy_unscaled = model_unscaled.evaluate(test_dataset2, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history_unscaled.history['accuracy'][np.max(history_unscaled.epoch)])\n",
    "print('Test accuracy:', test_accuracy_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: I can't see much of a difference here, to be honest. But I know it's supposed to take more training for the network to find the right parameters. However, since I'm constantly working with 5 epochs to reduce the time it takes to train each network, that might be why I can't really see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFc2HbEVCaXd"
   },
   "source": [
    "## Task 3: \n",
    "\n",
    "Use the reduced data set with 4 x 4 pixels and check the performance of your preferred convolution neural network on the reduced data set. Compare with all the previous results and comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: In order to do this, I first need to load the data anew to get it into the dimensions that I've used on previous labs when reducing the number of dimensions. Then I reduce the dimensions to 4, and then I run a model on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset3, metadata3 = tfds.load('fashion_mnist', as_supervised = True, with_info = True)\n",
    "train_dataset3, test_dataset3 = dataset3['train'], dataset3['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in memory\n",
    "train_dataset3 =  train_dataset3.cache()\n",
    "test_dataset3  =  test_dataset3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train_dataset3 =  train_dataset3.map(normalize)\n",
    "test_dataset3  =  test_dataset3.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating function for dimension reduction\n",
    "def reduce_size(images, labels):\n",
    "  images = tf.image.resize(images, [4,4])\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size to 4x4\n",
    "train_dataset_reduced = train_dataset3.map(reduce_size)\n",
    "test_dataset_reduced = test_dataset3.map(reduce_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.33333334 0.        ]\n",
      " [0.         0.5686275  0.2784314  0.        ]\n",
      " [0.         0.41568628 0.30980393 0.        ]\n",
      " [0.         0.         0.30980393 0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in train_dataset_reduced.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((4,4))\n",
    "print(image)\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.38431373 0.33333334 0.        ]\n",
      " [0.         0.42352942 0.4627451  0.        ]\n",
      " [0.         0.44705883 0.45882353 0.        ]\n",
      " [0.         0.42745098 0.34901962 0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a single image, and remove the color dimension by reshaping\n",
    "for image, label in test_dataset_reduced.take(1):\n",
    "  break\n",
    "image = image.numpy().reshape((4,4))\n",
    "print(image)\n",
    "\n",
    "# Plot the image - voila a piece of fashion clothing\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: As we can see above, the size reduction isn't keeping the information (as we as humans know it) very well when we only use 4x4. It will be interesting to see if there is a pattern that the network can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and create batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataset_reduced = train_dataset_reduced.repeat().shuffle(metadata3.splits['train'].num_examples).batch(BATCH_SIZE)\n",
    "test_dataset_reduced = test_dataset_reduced.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: Drawing on the experience from the previous experiments, I will build a model for the 4x4 data that is aimed to be as non-complex as possible. For example, I'll only use one Covd2D layer and one pooling layer. The last hidden layer will only have 20 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 19s 60ms/step - loss: 1.0299 - accuracy: 0.5963\n",
      "\n",
      "Train accuracy: 0.60445\n",
      "Test accuracy: 0.5963\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model4x4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(4, 4, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model4x4.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history4x4 = model4x4.fit(train_dataset_reduced,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=math.ceil(metadata3.splits['train'].num_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "test_loss4x4, test_accuracy4x4 = model4x4.evaluate(test_dataset_reduced, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', history4x4.history['accuracy'][np.max(history4x4.epoch)])\n",
    "print('Test accuracy:', test_accuracy4x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: It turns out that the network is quite bad at dealing with the data. The accuracy is a lot lower than for the models trained with examples of 28*28 pixel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.90595]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history1.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest train accuracy was: [0.97]\n",
      "It came from the model on list place: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sara/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  app.launch_new_instance()\n",
      "/Users/Sara/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "# Finding the highest train accuracy amongst all models\n",
    "all_train_accuracy = [[0.97],\n",
    "                      history1.history['accuracy'],\n",
    "                      history10.history['accuracy'][np.max(history10.epoch)],\n",
    "                      history220.history['accuracy'][np.max(history220.epoch)],\n",
    "                      history512.history['accuracy'][np.max(history512.epoch)],\n",
    "                      historyarch2.history['accuracy'][np.max(historyarch2.epoch)],\n",
    "                      historyarch3.history['accuracy'][np.max(historyarch3.epoch)],\n",
    "                      historyarch4.history['accuracy'][np.max(historyarch4.epoch)],\n",
    "                      historyA.history['accuracy'][np.max(historyA.epoch)],\n",
    "                      historyB.history['accuracy'][np.max(historyB.epoch)],\n",
    "                      historyC.history['accuracy'][np.max(historyC.epoch)],\n",
    "                      history_unscaled.history['accuracy'][np.max(history_unscaled.epoch)],\n",
    "                      history4x4.history['accuracy'][np.max(history4x4.epoch)]]\n",
    "\n",
    "print('Highest train accuracy was:', np.max(np.array(all_train_accuracy)))\n",
    "print('It came from the model on list place:', np.argmax(np.array(all_train_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sara/anaconda3/lib/python3.7/site-packages/numpy/lib/stride_tricks.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  args = [np.array(_m, copy=False, subok=subok) for _m in args]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 13 artists>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(all_train_accuracy)), height = all_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest test accuracy was: 0.9177\n",
      "It came from the model on list place: 0\n"
     ]
    }
   ],
   "source": [
    "# Finding the highest test accuracy amongst all models\n",
    "all_test_accuracy = [test_accuracy,\n",
    "                     test_accuracy1,\n",
    "                     test_accuracy10,\n",
    "                     test_accuracy220,\n",
    "                     test_accuracy512,\n",
    "                     test_accuracyarch2,\n",
    "                     test_accuracyarch3,\n",
    "                     test_accuracyarch4,\n",
    "                     test_accuracyA,\n",
    "                     test_accuracyB,\n",
    "                     test_accuracyC,\n",
    "                     test_accuracy_unscaled,\n",
    "                     test_accuracy4x4]\n",
    "\n",
    "print('Highest test accuracy was:', np.max(all_test_accuracy))\n",
    "print('It came from the model on list place:', np.argmax(all_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 13 artists>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(all_test_accuracy)), height = all_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: All models performed very equally, apart from the last one which was trained and tested on the low dim data. The very first model specified performed best in all cases, however, it was also trained with more epochs, so that might be why. To compare, I'll try my last model on the high-dimensional data and train it for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.2744 - accuracy: 0.9110\n",
      "\n",
      "Train accuracy: 0.94406664\n",
      "Test accuracy: 0.911\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "last_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "last_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "last_history = last_model.fit(train_dataset,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch=math.ceil(metadata3.splits['train'].num_examples/BATCH_SIZE),\n",
    "                    verbose=0,\n",
    "                    callbacks=[PrintDot()])\n",
    "\n",
    "last_test_loss, last_test_accuracy = last_model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nTrain accuracy:', last_history.history['accuracy'][np.max(last_history.epoch)])\n",
    "print('Test accuracy:', last_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary (as per requested as the main task)\n",
    "In this notebook I have built several deep learning models with the aim of classifying images of clothes. All models have performed with a very high accuracy, apart from the model trained and tested on low dimensional data. This is expected, since the NN structure makes very good use of high dimensional data and can detect patterns that other algorithms can't. Still though, a 59 % test accuracy on that data is surprisngly high, given that a naive chance classifier would only be expected to be right 10 % of the time due to the 10 categories.\n",
    "\n",
    "My overall impression from my experiments are that more complex network doesn't really (at least nessecerily) contribute to higher accuracy. However, too little complexity can reduce accuracy. This was shown in my last network, where I kept the complecity at a minimum (fewer nodes in last hidden layer, one one set of Conv2D + Pooling layers, etc.) while still using the full-dimensional data. This network didn't perform better than the original model, and only slightly better than most other models tried in this lab.\n",
    "\n",
    "Naturally, these results aren't very stable because they risk being very affected by the randomness of what we get in the train vs test data. From before, in previous labs, we know for example that the NN can be bad at predicting a sneaker if it is turned in the other direction on the picture, compared to the other sneakers. Since there are fewer examples on the sneaker turned in the \"wrong\" direction, we are more sensitive to where those pictures end up with regards to training and test set. As such, it is always better to cross-validate. In this case however, it was very clear that it was not possible to do cross-validation since it would've taken a veeery long time.\n",
    "\n",
    "In conclusion, neural networks are quite cool since they can obviously rather easily be trained to do a good job at classifying images, which \"normal\" statistical models often can't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grader box: \n",
    "\n",
    "In what follows the grader will put the values according the following check list:\n",
    "\n",
    "* 1 Have all commands included in a raw notebook been evaluated? (0 or 0.5pt)\n",
    "* 2 Have all commands been experimented with? (0 or 0.5pt)\n",
    "* 3 Have all experiments been briefly commented? (0 or 0.5pt)\n",
    "* 4 Have all tasks been attempted? (0, 0.5, or 1pt)\n",
    "* 5 How many of the tasks have been completed? (0, 0.5, or 1pt)\n",
    "* 6 How many of the tasks (completed or not) have been commented? (0, 0.5, or 1pt)\n",
    "* 7 Have been the conclusions from performing the tasks clearly stated? (0, 0.5, or 1pt)\n",
    "* 8 Have been the overall organization of the submitted Lab notebook been neat and easy to follow by the grader? (0, or 0.5pt) \n",
    "\n",
    "\n",
    "#### 1 Have all commands included in a raw notebook been evaluated? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 2 Have all commands been experimented with? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 3 Have all experiments been briefly commented? (0 or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 4 Have all tasks been attempted? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 5 How many of the tasks have been completed? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 6 How many of the tasks (completed or not) have been commented? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 7 Have been the conclusions from performing the tasks clearly stated? (0, 0.5, or 1pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "#### 8 Have been the overall organization of the submitted Lab notebook been neat and easy to follow by the grader? (0, or 0.5pt)\n",
    "\n",
    "#### Grader's comment (if desired): \n",
    "N/A\n",
    "\n",
    "### Overall score\n",
    "\n",
    "### Score and grader's comment (if desired): \n",
    "N/A"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4Student_Buelent_v3.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
